headers:
  llamacpp:
    Content-Type: application/json

muxing:
  mux_url: http://127.0.0.1:8989/v1/mux/
  trimm_from_testcase_url: http://127.0.0.1:8989/llamacpp/
  provider_endpoint:
    url: http://127.0.0.1:8989/api/v1/provider-endpoints
    headers:
      Content-Type: application/json
    data: |
      {
        "name": "llamacpp_muxing",
        "description": "Muxing testing endpoint",
        "provider_type": "llamacpp",
        "endpoint": "./codegate_volume/models",
        "auth_type": "none"
      }
  muxes:
    url: http://127.0.0.1:8989/api/v1/workspaces/default/muxes
    headers:
      Content-Type: application/json
    rules:
      - model: qwen2.5-coder-0.5b-instruct-q5_k_m
        matcher_type: catch_all
        matcher: ""

testcases:
  llamacpp_chat:
    name: LlamaCPP Chat
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder-0.5b-instruct-q5_k_m",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  llamacpp_fim:
    name: LlamaCPP FIM
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/completions
    data: |
      {
        "model": "qwen2.5-coder-0.5b-instruct-q5_k_m",
        "max_tokens": 4096,
        "temperature": 0,
        "stream": true,
        "stop": ["<|endoftext|>", "<|fim_prefix|>", "<|fim_middle|>", "<|fim_suffix|>", "<|fim_pad|>", "<|repo_name|>", "<|file_sep|>", "<|im_start|>", "<|im_end|>", "/src/", "#- coding: utf-8", "```", "def test"],
        "prompt":"# Do not add comments\n<|fim_prefix|>\n# codegate/greet.py\ndef print_hello():\n    <|fim_suffix|>\n\n\nprint_hello()\n<|fim_middle|>"
      }
    likes: |
      print("Hello, World!")

  llamacpp_malicious_package_question:
    name: Llamacpp Malicious Package
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content": "is it safe to use invokehttp?",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder-0.5b-instruct-q5_k_m",
        "stream":true,
        "temperature":0
      }
    contains: |
      CodeGate detected one or more malicious, deprecated or archived packages.

  llamacpp_secret_redacting_chat:
    name: LlamaCPP secret redacting chat
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Here's my API key: ghp_aBcDeFgHiJkLmNoPqRsTuVwXyZ0123456789. Can you help me list my repos on GitHub?",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder-0.5b-instruct-q5_k_m",
        "stream":true,
        "temperature":0
      }
    contains: |
      CodeGate prevented 1 secret
    not_contains: |
      ghp_aBcDeFgHiJkLmNoPqRsTuVwXyZ0123456789
