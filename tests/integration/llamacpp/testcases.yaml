headers:
  llamacpp:
    Content-Type: application/json

testcases:
  llamacpp_chat:
    name: LlamaCPP Chat
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder-0.5b-instruct-q5_k_m",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  llamacpp_fim:
    name: LlamaCPP FIM
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/completions
    data: |
      {
        "model": "qwen2.5-coder-0.5b-instruct-q5_k_m",
        "max_tokens": 4096,
        "temperature": 0,
        "stream": true,
        "stop": ["<|endoftext|>", "<|fim_prefix|>", "<|fim_middle|>", "<|fim_suffix|>", "<|fim_pad|>", "<|repo_name|>", "<|file_sep|>", "<|im_start|>", "<|im_end|>", "/src/", "#- coding: utf-8", "```", "def test"],
        "prompt":"# Do not add comments\n<|fim_prefix|>\n# codegate/greet.py\ndef print_hello():\n    <|fim_suffix|>\n\n\nprint_hello()\n<|fim_middle|>"
      }
    likes: |
      print("Hello, World!")

