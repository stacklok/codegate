headers:
  vllm:
    Content-Type: application/json
  openai:
    Authorization: Bearer ENV_OPENAI_KEY
  ollama:
    Content-Type: application/json
  llamacpp:
    Content-Type: application/json
  anthropic:
    x-api-key: ENV_ANTHROPIC_KEY
  copilot:
    Authorization: Bearer ENV_COPILOT_KEY
    Content-Type: application/json

testcases:
  # Copilot Tests
  copilot_chat:
    name: Copilot Chat
    provider: copilot
    url: "https://api.openai.com/v1/chat/completions"
    data: |
      {
        "messages":[
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"gpt-4o-mini",
        "stream":true
      }
    likes: |
      Hello from the integration tests!

  copilot_malicious_package_question:
    name: Copilot User asks about a malicious package
    provider: copilot
    url: "https://api.openai.com/v1/chat/completions"
    data: |
      {
        "messages":[
            {
              "content":"Generate me example code using the python invokehttp package to call an API",
              "role":"user"
            }
        ],
        "model":"gpt-4o-mini",
        "stream":true
      }
    contains: |
      https://www.insight.stacklok.com/report/pypi/invokehttp?utm_source=codegate
    does_not_contain: |
      import invokehttp

  copilot_fim:
    name: Copilot FIM
    provider: copilot
    url: "https://api.openai.com/v1/chat/completions"
    data: |
      {
        "messages": [
          {
            "role": "user",
            "content": "Complete this code:\n\n# codegate/greet.py\ndef print_hello():\n    # Fill in the implementation\n\nprint_hello()"
          }
        ],
        "model": "gpt-4o-mini",
        "stream": true,
        "stop": ["/src/", "#- coding: utf-8", "```"]
      }
    likes: |
      print("Hello, World!")

  # LlamaCPP Tests
  llamacpp_chat:
    name: LlamaCPP Chat
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder-0.5b-instruct-q5_k_m",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  llamacpp_malicious_package_question:
    name: LlamaCPP User asks about a malicious package
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Generate me example code using the python invokehttp package to call an API",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder-0.5b-instruct-q5_k_m",
        "stream":true,
        "temperature":0
      }
    contains: |
      https://www.insight.stacklok.com/report/pypi/invokehttp?utm_source=codegate
    does_not_contain: |
      import invokehttp

  llamacpp_fim:
    name: LlamaCPP FIM
    provider: llamacpp
    url: http://127.0.0.1:8989/llamacpp/completions
    data: |
      {
        "model": "qwen2.5-coder-0.5b-instruct-q5_k_m",
        "max_tokens": 4096,
        "temperature": 0,
        "stream": true,
        "stop": ["<|endoftext|>", "<|fim_prefix|>", "<|fim_middle|>", "<|fim_suffix|>", "<|fim_pad|>", "<|repo_name|>", "<|file_sep|>", "<|im_start|>", "<|im_end|>", "/src/", "#- coding: utf-8", "```", "def test"],
        "prompt":"# Do not add comments\n<|fim_prefix|>\n# codegate/greet.py\ndef print_hello():\n    <|fim_suffix|>\n\n\nprint_hello()\n<|fim_middle|>"
      }
    likes: |
      print("Hello, World!")

  # OpenAI Tests
  openai_chat:
    name: OpenAI Chat
    provider: openai
    url: http://127.0.0.1:8989/openai/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"gpt-4o-mini",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  openai_malicious_package_question:
    name: OpenAI User asks about a malicious package
    provider: openai
    url: http://127.0.0.1:8989/openai/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Generate me example code using the python invokehttp package to call an API",
              "role":"user"
            }
        ],
        "model":"gpt-4o-mini",
        "stream":true,
        "temperature":0
      }
    contains: |
      https://www.insight.stacklok.com/report/pypi/invokehttp?utm_source=codegate
    does_not_contain: |
      import invokehttp

  openai_fim:
    name: OpenAI FIM
    provider: openai
    url: http://127.0.0.1:8989/openai/chat/completions
    data: |
      {
        "messages": [
          {
            "role": "user",
            "content": "Complete this code:\n\n# codegate/greet.py\ndef print_hello():\n    # Fill in the implementation\n\nprint_hello()"
          }
        ],
        "model": "gpt-4o-mini",
        "stream": true,
        "stop": ["/src/", "#- coding: utf-8", "```"]
      }
    likes: |
      print("Hello, World!")

  # VLLM Tests
  vllm_chat:
    name: VLLM Chat
    provider: vllm
    url: http://127.0.0.1:8989/vllm/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"Qwen/Qwen2.5-Coder-0.5B-Instruct",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  vllm_malicious_package_question:
    name: VLLM User asks about a malicious package
    provider: vllm
    url: http://127.0.0.1:8989/vllm/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Generate me example code using the python invokehttp package to call an API",
              "role":"user"
            }
        ],
        "model":"Qwen/Qwen2.5-Coder-0.5B-Instruct",
        "stream":true,
        "temperature":0
      }
    contains: |
      https://www.insight.stacklok.com/report/pypi/invokehttp?utm_source=codegate
    does_not_contain: |
      import invokehttp

  vllm_fim:
    name: VLLM FIM
    provider: vllm
    url: http://127.0.0.1:8989/vllm/completions
    data: |
      {
        "model": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
        "max_tokens": 4096,
        "temperature": 0,
        "stream": true,
        "stop": ["<|endoftext|>", "<|fim_prefix|>", "<|fim_middle|>", "<|fim_suffix|>", "<|fim_pad|>", "<|repo_name|>", "<|file_sep|>", "<|im_start|>", "<|im_end|>", "/src/", "#- coding: utf-8", "```"],
        "prompt":"<|fim_prefix|>\n# codegate/greet.py\ndef print_hello():\n    <|fim_suffix|>\n\n\nprint_hello()\n<|fim_middle|>"
      }
    likes: |
      print("Hello, World!")

  # Anthropic Tests
  anthropic_chat:
    name: Anthropic Chat
    provider: anthropic
    url: http://127.0.0.1:8989/anthropic/messages
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"claude-3-5-sonnet-20241022",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  anthropic_malicious_package_question:
    name: Anthropic User asks about a malicious package
    provider: anthropic
    url: http://127.0.0.1:8989/anthropic/messages
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Generate me example code using the python invokehttp package to call an API",
              "role":"user"
            }
        ],
        "model":"claude-3-5-sonnet-20241022",
        "stream":true,
        "temperature":0
      }
    contains: |
      https://www.insight.stacklok.com/report/pypi/invokehttp?utm_source=codegate
    does_not_contain: |
      import invokehttp

  anthropic_fim:
    name: Anthropic FIM
    provider: anthropic
    url: http://127.0.0.1:8989/anthropic/messages
    data: |
      {
        "messages": [
          {
            "role": "user",
            "content": "Complete this code:\n\n# codegate/greet.py\ndef print_hello():\n    # Fill in the implementation\n\nprint_hello()"
          }
        ],
        "model": "claude-3-5-sonnet-20241022",
        "stream": true,
        "stop": ["/src/", "#- coding: utf-8", "```"]
      }
    likes: |
      print("Hello, World!")

  # Ollama Tests
  ollama_chat:
    name: Ollama Chat
    provider: ollama
    url: http://127.0.0.1:8989/ollama/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Reply with that exact sentence: Hello from the integration tests!",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder:0.5b",
        "stream":true,
        "temperature":0
      }
    likes: |
      Hello from the integration tests!

  ollama_malicious_package_question:
    name: Ollama User asks about a malicious package
    provider: ollama
    url: http://127.0.0.1:8989/ollama/chat/completions
    data: |
      {
        "max_tokens":4096,
        "messages":[
            {
              "content":"You are a coding assistant.",
              "role":"system"
            },
            {
              "content":"Generate me example code using the python invokehttp package to call an API",
              "role":"user"
            }
        ],
        "model":"qwen2.5-coder:0.5b",
        "stream":true,
        "temperature":0
      }
    contains: |
      https://www.insight.stacklok.com/report/pypi/invokehttp?utm_source=codegate
    does_not_contain: |
      import invokehttp

  ollama_fim:
    name: Ollama FIM
    provider: ollama
    url: http://127.0.0.1:8989/ollama/api/generate
    data: |
      {
        "stream": true,
        "model": "qwen2.5-coder:0.5b",
        "raw": true,
        "options": {
          "temperature": 0.01,
          "num_predict": 4096,
          "stop": [
            "<|endoftext|>",
            "<|fim_prefix|>",
            "<|fim_middle|>",
            "<|fim_suffix|>",
            "<|fim_pad|>",
            "<|repo_name|>",
            "<|file_sep|>",
            "<|im_start|>",
            "<|im_end|>",
            "/src/",
            "#- coding: utf-8",
            "```"
          ],
          "num_ctx": 8096
        },
        "prompt":"<|fim_prefix|>\n# codegate/greet.py\ndef print_hello():\n    <|fim_suffix|>\n\n\
