# This workflow will run the integration tests for the project
name: Tests - Integration

on:
  workflow_call:
    inputs:
      artifact-name:
        description: 'The name of the artifact to download'
        required: true
        type: string
    secrets:
      copilot-key:
        description: 'The Copilot key to use for integration tests'
        required: true
      anthropic-key:
        description: 'The Anthropic key to use for integration tests'
        required: true

jobs:
  integration-tests:
    name: Run
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [ "3.12" ]
    env:
      CONTAINER_NAME: "codegate"
      CERT_FILE: "/app/codegate_volume/certs/ca.crt"
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          lfs: true

      - name: Checkout LFS objects
        run: git lfs pull

      - name: Ensure file permissions for mounted volume
        run: |
          mkdir -p ./codegate_volume/certs ./codegate_volume/models ./codegate_volume/db
          chmod -R 777 ./codegate_volume

      - name: Download Docker image artifact
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16 # v4
        with:
          name: ${{ inputs.artifact-name }}

      - name: Load Docker image
        run: |
          docker load -i image.tar
          echo "Loaded image:"
          docker images

      - name: Download the Qwen2.5-Coder-0.5B-Instruct-GGUF model
        run: |
          # This is needed for the llamacpp integration tests
          wget -P ./codegate_volume/models https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF/resolve/main/qwen2.5-coder-0.5b-instruct-q5_k_m.gguf

      - name: Run container from the loaded image
        run: |
          # Get the image name
          DOCKER_IMAGE=$(docker images --format "{{.Repository}}:{{.Tag}}" | head -n 1)
          echo "Running container from image: $DOCKER_IMAGE"

          # Run the container
          docker run --name $CONTAINER_NAME -d --network host \
            -v "$(pwd)"/codegate_volume:/app/codegate_volume \
            -e CODEGATE_APP_LOG_LEVEL=DEBUG \
            -e CODEGATE_OLLAMA_URL=http://localhost:11434 \
            -e CODEGATE_VLLM_URL=http://localhost:8000 \
            --restart unless-stopped $DOCKER_IMAGE

          # Confirm the container started
          echo "Container started:"
          docker ps

          # Verify container is running with correct ports
          docker ps -f name=$CONTAINER_NAME

          # Check mount configuration
          docker inspect $CONTAINER_NAME -f '{{ json .Mounts }}' | jq

      - name: Ensure certificates are available in the container
        timeout-minutes: 4
        run: |
          # Wait for the cert file to be available in the container
          while true; do
            echo "Checking for $CERT_FILE in container $CONTAINER_NAME..."

            if docker exec "$CONTAINER_NAME" test -f "$CERT_FILE"; then
              echo "Cert file found: $CERT_FILE"
              break
            else
              echo "Cert file not found. Retrying in 5 seconds..."
              sleep 5
            fi
          done

          # Verify volume contents are accessible
          docker exec $CONTAINER_NAME ls -la /app/codegate_volume

          # Print the container logs we got so far
          docker logs $CONTAINER_NAME

      - name: Install the CodeGate certificate
        run: |
          docker cp codegate:/app/codegate_volume/certs/ca.crt ./codegate.crt
          sudo cp ./codegate.crt /usr/local/share/ca-certificates/codegate.crt
          sudo update-ca-certificates

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install Poetry
        uses: snok/install-poetry@76e04a911780d5b312d89783f7b1cd627778900a # v1
        with:
          version: 2.0.1
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        run: poetry install --with dev

      - name: Run integration tests - Copilot
        env:
          CODEGATE_PROVIDERS: "copilot"
          CA_CERT_FILE: "/home/runner/work/codegate/codegate/codegate_volume/certs/ca.crt"
          ENV_COPILOT_KEY: ${{ secrets.copilot-key }}
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Run integration tests - OpenAI
        env:
          CODEGATE_PROVIDERS: "openai"
          ENV_OPENAI_KEY: ${{ secrets.copilot-key }} # We use the same key for OpenAI as the Copilot tests
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Run integration tests - Anthropic
        env:
          CODEGATE_PROVIDERS: "anthropic"
          ENV_ANTHROPIC_KEY: ${{ secrets.anthropic-key }}
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Run Ollama
        run: |
          docker run -d -v ollama:/root/.ollama --network host --name ollama ollama/ollama
          docker ps -f name=ollama
          echo "Loop until the endpoint responds successfully"
          while ! curl --silent --fail --get "http://localhost:11434" >/dev/null; do
            echo "Ollama not available yet. Retrying in 2 seconds..."
            sleep 2
          done
          echo "Ollama is now available!"
          docker exec -d ollama ollama run qwen2.5-coder:0.5b

          sleep 120 # Sleep for 2 minutes to allow Ollama to download the model. TODO: Improve this
          docker logs ollama

          # Verify the Ollama API is working
          curl http://localhost:11434/api/generate -d '{
            "model": "qwen2.5-coder:0.5b",
            "prompt": "Why is the sky blue?",
            "stream": false
          }'

          docker logs ollama

      - name: Run integration tests - Ollama
        env:
          CODEGATE_PROVIDERS: "ollama"
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Print the Ollama container logs (useful for debugging)
        if: always()
        run: |
          docker logs ollama

      - name: Build and run the vllm container
        run: |
          git clone https://github.com/vllm-project/vllm.git
          cd vllm
          docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .
          docker run -d  --name vllm \
             --network="host" \
             vllm-cpu-env --model Qwen/Qwen2.5-Coder-0.5B-Instruct

      - name: Verify the vllm container is running
        run: |
          echo -e "\nVerify the vllm container is serving\n"
          docker ps -f name=vllm

          echo "Loop until the endpoint responds successfully"
          while ! curl --silent --fail --get "http://localhost:8000/ping" >/dev/null; do
            echo "Ping not available yet. Retrying in 2 seconds..."
            sleep 2
          done
          echo -e "\nPing is now available!\n"

          echo -e "\nVerify the completions endpoint works\n"
          curl http://localhost:8000/v1/completions -H "Content-Type: application/json"   -d '{
              "model": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
              "prompt": ["How to make pizza"],
              "max_tokens": 100,
              "temperature": 0
            }'

          echo -e "\nVerify the chat/completions endpoint works\n"
          curl -X POST http://localhost:8000/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
                "messages": [
                  {"role": "system", "content": "You are a coding assistant."},
                  {"role": "user", "content": "Hello"}
                ],
                "temperature": 0,
                "max_tokens": 4096,
                "extra_body": {}
              }'

          # Print a new line and then the message in a single echo
          echo -e "\nPrint the vllm container logs\n"
          docker logs vllm

      - name: Run integration tests - vllm
        env:
          CODEGATE_PROVIDERS: "vllm"
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Run integration tests - llamacpp
        env:
          CODEGATE_PROVIDERS: "llamacpp"
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Print the CodeGate container logs (useful for debugging)
        if: always()
        run: |
          docker logs $CONTAINER_NAME
          echo "Models contents:"
          ls -la codegate_volume/models
          docker exec $CONTAINER_NAME ls -la /app/codegate_volume/models
          echo "Certs contents:"
          ls -la codegate_volume/certs
          docker exec $CONTAINER_NAME ls -la /app/codegate_volume/certs
          echo "DB contents:"
          ls -la codegate_volume/db
          docker exec $CONTAINER_NAME ls -la /app/codegate_volume/db

      - name: Print the vllm container logs (useful for debugging)
        if: always()
        run: |
          docker logs vllm
