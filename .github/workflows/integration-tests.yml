# This workflow will run the integration tests for the project
name: Tests - Integration

on:
  workflow_call:
    inputs:
      artifact-name:
        description: 'The name of the artifact to download'
        required: true
        type: string
    secrets:
      copilot-key:
        description: 'The Copilot key to use for integration tests'
        required: true
      anthropic-key:
        description: 'The Anthropic key to use for integration tests'
        required: true

jobs:
  integration-tests:
    name: Test
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false # Continue running other tests if one fails
      matrix:
        python-version: [ "3.12" ]
        test-provider: [ "copilot", "openai", "anthropic", "ollama", "vllm", "llamacpp" ]
    env:
      ENV_COPILOT_KEY: ${{ secrets.copilot-key }}
      ENV_OPENAI_KEY: ${{ secrets.copilot-key }} # We use the same key for OpenAI as the Copilot tests
      ENV_ANTHROPIC_KEY: ${{ secrets.anthropic-key }}
      CA_CERT_FILE: "/home/runner/work/codegate/codegate/codegate_volume/certs/ca.crt"
      CODEGATE_CONTAINER_NAME: "codegate"
      CODEGATE_MOUNT_PATH_CERT_FILE: "/app/codegate_volume/certs/ca.crt"
      CODEGATE_LOG_LEVEL: "DEBUG"
      LOCAL_OLLAMA_URL: "http://localhost:11434"
      LOCAL_VLLM_URL: "http://localhost:8000"
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          lfs: true

      - name: Checkout LFS objects
        run: git lfs pull

      - name: Ensure file permissions for mounted volume
        run: |
          mkdir -p ./codegate_volume/certs ./codegate_volume/models ./codegate_volume/db
          chmod -R 777 ./codegate_volume

      - name: Download the CodeGate container image
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16 # v4
        with:
          name: ${{ inputs.artifact-name }}

      - name: Load the CodeGate container image
        run: |
          docker load -i image.tar
          echo "Loaded image:"
          docker images

      - name: Download the Qwen2.5-Coder-0.5B-Instruct-GGUF model (llamacpp only)
        if: ${{ matrix.test-provider == 'llamacpp' }} # This is only needed for llamacpp
        run: |
          wget -P ./codegate_volume/models https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF/resolve/main/qwen2.5-coder-0.5b-instruct-q5_k_m.gguf

      - name: Start the CodeGate container
        run: |
          # Get the image name
          DOCKER_IMAGE=$(docker images --format "{{.Repository}}:{{.Tag}}" | head -n 1)
          echo "Running container from image: $DOCKER_IMAGE"

          # Run the container
          docker run --name $CODEGATE_CONTAINER_NAME -d --network host \
            -v "$(pwd)"/codegate_volume:/app/codegate_volume \
            -e CODEGATE_APP_LOG_LEVEL=$CODEGATE_LOG_LEVEL \
            -e CODEGATE_OLLAMA_URL=$LOCAL_OLLAMA_URL \
            -e CODEGATE_VLLM_URL=$LOCAL_VLLM_URL \
            --restart unless-stopped $DOCKER_IMAGE

          # Confirm the container started
          echo "Container started:"
          docker ps

          # Verify container is running with correct ports
          docker ps -f name=$CODEGATE_CONTAINER_NAME

          # Check mount configuration
          docker inspect $CODEGATE_CONTAINER_NAME -f '{{ json .Mounts }}' | jq

      - name: Ensure certificates are available in the container
        timeout-minutes: 4
        run: |
          # Wait for the cert file to be available in the container
          while true; do
            echo "Checking for $CODEGATE_MOUNT_PATH_CERT_FILE in container $CODEGATE_CONTAINER_NAME..."

            if docker exec "$CODEGATE_CONTAINER_NAME" test -f "$CODEGATE_MOUNT_PATH_CERT_FILE"; then
              echo "Cert file found: $CODEGATE_MOUNT_PATH_CERT_FILE"
              break
            else
              echo "Cert file not found. Retrying in 5 seconds..."
              sleep 5
            fi
          done

          # Verify volume contents are accessible
          docker exec $CODEGATE_CONTAINER_NAME ls -la /app/codegate_volume

      - name: Copy and install the CodeGate certificate
        run: |
          docker cp codegate:/app/codegate_volume/certs/ca.crt ./codegate.crt
          sudo cp ./codegate.crt /usr/local/share/ca-certificates/codegate.crt
          sudo update-ca-certificates

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install Poetry
        uses: snok/install-poetry@76e04a911780d5b312d89783f7b1cd627778900a # v1
        with:
          version: 2.0.1
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        run: poetry install --with dev

      - name: Run the Ollama container (ollama-only)
        if: ${{ matrix.test-provider == 'ollama' }} # This is only needed for Ollama
        run: |
          docker run -d -v ollama:/root/.ollama --network host --name ollama ollama/ollama
          docker ps -f name=ollama
          echo "Loop until the endpoint responds successfully"
          while ! curl --silent --fail --get "http://localhost:11434" >/dev/null; do
            echo "Ollama not available yet. Retrying in 2 seconds..."
            sleep 2
          done
          echo "Ollama is now available!"

          # Run the model
          docker exec -d ollama ollama run qwen2.5-coder:0.5b

          echo "Waiting for model to be ready..."
          while true; do
            # Try to make a test query to the model
            response=$(curl -s http://localhost:11434/api/generate -d '{
              "model": "qwen2.5-coder:0.5b",
              "prompt": "Why is the sky blue?",
              "stream": false
            }' 2>&1)

            # Check if the response contains an error
            if echo "$response" | grep -q "error"; then
              echo "Model not ready yet. Retrying in 5 seconds..."
              sleep 5
            else
              echo "Model is ready!"
              break
            fi
          done

          # Verify the Ollama API is working
          curl http://localhost:11434/api/generate -d '{
            "model": "qwen2.5-coder:0.5b",
            "prompt": "Why is the sky blue?",
            "stream": false
          }'

      - name: Build and run the vllm container (vllm-only)
        if: ${{ matrix.test-provider == 'vllm' }} # This is only needed for VLLM
        run: |
          # We clone the VLLM repo and build the container because the CPU-mode container is not published
          git clone https://github.com/vllm-project/vllm.git
          cd vllm
          docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .
          docker run -d  --name vllm \
             --network="host" \
             vllm-cpu-env --model Qwen/Qwen2.5-Coder-0.5B-Instruct

          echo -e "\nVerify the vllm container is serving\n"
          docker ps -f name=vllm

          echo "Loop until the endpoint responds successfully"
          while ! curl --silent --fail --get "http://localhost:8000/ping" >/dev/null; do
            echo "Ping not available yet. Retrying in 2 seconds..."
            sleep 2
          done
          echo -e "\nPing is now available!\n"

          echo -e "\nVerify the completions endpoint works\n"
          curl http://localhost:8000/v1/completions -H "Content-Type: application/json"   -d '{
              "model": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
              "prompt": ["How to make pizza"],
              "max_tokens": 100,
              "temperature": 0
            }'

          echo -e "\nVerify the chat/completions endpoint works\n"
          curl -X POST http://localhost:8000/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
                "messages": [
                  {"role": "system", "content": "You are a coding assistant."},
                  {"role": "user", "content": "Hello"}
                ],
                "temperature": 0,
                "max_tokens": 4096,
                "extra_body": {}
              }'

          echo -e "\nPrint the vllm container logs\n"
          docker logs vllm

      - name: Tests - ${{ matrix.test-provider }}
        env:
          CODEGATE_PROVIDERS: ${{ matrix.test-provider }}
        run: |
          poetry run python tests/integration/integration_tests.py

      - name: Print the CodeGate container logs
        if: always()
        run: |
          docker logs $CODEGATE_CONTAINER_NAME
          echo "Models contents:"
          ls -la codegate_volume/models
          docker exec $CODEGATE_CONTAINER_NAME ls -la /app/codegate_volume/models
          echo "Certs contents:"
          ls -la codegate_volume/certs
          docker exec $CODEGATE_CONTAINER_NAME ls -la /app/codegate_volume/certs
          echo "DB contents:"
          ls -la codegate_volume/db
          docker exec $CODEGATE_CONTAINER_NAME ls -la /app/codegate_volume/db

      - name: Print the vllm container logs (vllm-only)
        if: ${{ matrix.test-provider == 'vllm' }} # This is only needed for VLLM
        run: |
          docker logs vllm

      - name: Print the Ollama container logs (ollama-only)
        if: ${{ matrix.test-provider == 'ollama' }} # This is only needed for Ollama
        run: |
          docker logs ollama
