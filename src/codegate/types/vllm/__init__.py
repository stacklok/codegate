# VLLM types and generators are mainly a repackaging of OpenAI ones,
# except for a few types. To keep things simple, we repackage all used
# structs, but retain the right (and duty) to clone the structs in
# this package at the first signal of divergence.

from codegate.types.openai import (
    URL,
    AssistantMessage,
    Audio,
    AudioContent,
    # types
    AudioMessage,
    ChatCompletion,
    ChatCompletionRequest,
    Choice,
    ChoiceDelta,
    CompletionTokenDetails,
    DeveloperMessage,
    FunctionCall,
    FunctionChoice,
    FunctionDef,
    FunctionMessage,
    ImageContent,
    InputAudio,
    JsonSchema,
    LegacyCompletionRequest,
    LegacyFunctionDef,
    LogProbs,
    LogProbsContent,
    Message,
    MessageDelta,
    PromptTokenDetails,
    RawLogProbsContent,
    RefusalContent,
    ResponseFormat,
    ServiceTier,
    StaticContent,
    StreamingChatCompletion,
    StreamOption,
    SystemMessage,
    TextContent,
    ToolCall,
    ToolChoice,
    ToolDef,
    ToolMessage,
    Usage,
    UserMessage,
    # generators
    completions_streaming,
    message_wrapper,
    stream_generator,
)

from ._response_models import (
    VllmMessageError,
)

__all__ = [
    "URL",
    "AssistantMessage",
    "Audio",
    "AudioContent",
    "AudioMessage",
    "ChatCompletion",
    "ChatCompletionRequest",
    "Choice",
    "ChoiceDelta",
    "CompletionTokenDetails",
    "DeveloperMessage",
    "FunctionCall",
    "FunctionChoice",
    "FunctionDef",
    "FunctionMessage",
    "ImageContent",
    "InputAudio",
    "JsonSchema",
    "LegacyCompletionRequest",
    "LegacyFunctionDef",
    "LogProbs",
    "LogProbsContent",
    "Message",
    "MessageDelta",
    "PromptTokenDetails",
    "RawLogProbsContent",
    "RefusalContent",
    "ResponseFormat",
    "ServiceTier",
    "StaticContent",
    "StreamingChatCompletion",
    "StreamOption",
    "SystemMessage",
    "TextContent",
    "ToolCall",
    "ToolChoice",
    "ToolDef",
    "ToolMessage",
    "Usage",
    "UserMessage",
    "completions_streaming",
    "message_wrapper",
    "stream_generator",
    "VllmMessageError",
]
